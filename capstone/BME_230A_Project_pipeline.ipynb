{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1P3qVYhzdjCe",
        "DJ1dZj8QgZWe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "-DhlWd4TdcMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    # Running on Google Colab, so install Biopython first\n",
        "    !pip install biopython\n",
        "\n",
        "except ImportError:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKO48n9lZU9w",
        "outputId": "1a0c37a2-85ad-4524-bb2b-cfd29e652dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.25.2)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "import requests #Used for HTTP request\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from sklearn import preprocessing\n",
        "from collections import Counter\n",
        "from Bio import Entrez, SeqIO, ExPASy, SwissProt, GenBank\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "\n"
      ],
      "metadata": {
        "id": "YlFi5uBHfPzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Data\n"
      ],
      "metadata": {
        "id": "1P3qVYhzdjCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fetch Phage Data from PhagesDB\n",
        "def fetch_phages_data():\n",
        "    url = 'https://phagesdb.org/api/phages/'\n",
        "    params = {\n",
        "        'page': 1,\n",
        "        'page_size': 5  # Adjust page_size as needed to fetch more data if necessary\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        # Filter out phages where sequencing is not finished\n",
        "        complete_phages = [phage for phage in data['results'] if phage.get('seq_finished') is True]\n",
        "        return {'results': complete_phages, 'count': data['count'], 'next': data['next'], 'previous': data['previous']}\n",
        "    else:\n",
        "        print(f\"Failed to fetch data: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Step 2: Parse GenBank Accession Numbers\n",
        "def parse_genbank_accession(phages_data):\n",
        "    accession_numbers = []\n",
        "    for phage in phages_data['results']:\n",
        "        if 'genbank_accession' in phage and phage['genbank_accession']:\n",
        "            accession_numbers.append(phage['genbank_accession'])\n",
        "    return accession_numbers\n",
        "\n",
        "# Step 3: Fetch Detailed Information from GenBank\n",
        "def fetch_genbank_data(accession_numbers):\n",
        "    all_proteins = []\n",
        "    all_protein_ids = []\n",
        "    for accession in accession_numbers:\n",
        "        url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
        "        params = {\n",
        "            'db': 'nucleotide',\n",
        "            'id': accession,\n",
        "            'rettype': 'gb',\n",
        "            'retmode': 'text'\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "\n",
        "            genbank_data = response.text\n",
        "            non_hypothetical_proteins = extract_non_hypothetical_proteins(genbank_data)\n",
        "            all_proteins.extend(non_hypothetical_proteins)\n",
        "        else:\n",
        "            print(f\"Failed to fetch data for {accession}: {response.status_code}\")\n",
        "\n",
        "    print(accession_numbers)\n",
        "    for protein in all_proteins:\n",
        "        p_id = protein.get('protein_id')\n",
        "        p_product = protein.get('product')\n",
        "        print(p_id, p_product)\n",
        "        all_protein_ids.append(p_id)\n",
        "\n",
        "    return all_protein_ids\n",
        "\n",
        "\n",
        "def extract_non_hypothetical_proteins(genbank_data):\n",
        "    lines = genbank_data.split(\"\\n\")\n",
        "    proteins = []\n",
        "    capturing = False\n",
        "    current_protein = {}\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"CDS\"):\n",
        "            if current_protein:  # save the previous one if not hypothetical\n",
        "                if current_protein.get('product') and \"hypothetical protein\" not in current_protein['product']:\n",
        "                    proteins.append(current_protein)\n",
        "            current_protein = {'location': line.split()[1]}  # reset for the new CDS\n",
        "            capturing = True\n",
        "        elif capturing:\n",
        "            if line.startswith(\"/\"):\n",
        "                key, value = line.split(\"=\")[0], \"=\".join(line.split(\"=\")[1:])\n",
        "                key = key.replace(\"/\", \"\").strip()\n",
        "                value = value.replace('\"', '').strip()\n",
        "                current_protein[key] = value\n",
        "            elif line.startswith(\"ORIGIN\") or line.startswith(\"gene\") and current_protein:  # end of CDS entry\n",
        "                if 'product' in current_protein and \"hypothetical protein\" not in current_protein['product']:\n",
        "                    proteins.append(current_protein)\n",
        "                capturing = False\n",
        "                current_protein = {}\n",
        "\n",
        "    # Check for the last CDS entry\n",
        "    if current_protein and 'product' in current_protein and \"hypothetical protein\" not in current_protein['product']:\n",
        "        proteins.append(current_protein)\n",
        "\n",
        "    return proteins\n",
        "\n",
        "'''\n",
        "# Main function to orchestrate the fetching and processing\n",
        "def main():\n",
        "    phages_data = fetch_phages_data()\n",
        "    if phages_data:\n",
        "        accession_numbers = parse_genbank_accession(phages_data)\n",
        "        if accession_numbers:\n",
        "            fetch_genbank_data(accession_numbers)\n",
        "        else:\n",
        "            print(\"No accession numbers found.\")\n",
        "    else:\n",
        "        print(\"No phage data to process.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7YKGfvQQdmKk",
        "outputId": "0c979a28-4905-4deb-88f8-c42c8c5235dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Main function to orchestrate the fetching and processing\\ndef main():\\n    phages_data = fetch_phages_data()\\n    if phages_data:\\n        accession_numbers = parse_genbank_accession(phages_data)\\n        if accession_numbers:\\n            fetch_genbank_data(accession_numbers)\\n        else:\\n            print(\"No accession numbers found.\")\\n    else:\\n        print(\"No phage data to process.\")\\n\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chemical Analysis stats"
      ],
      "metadata": {
        "id": "sm7MSqPL_Nfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Chemi_stats = {\n",
        "                   'A':{'C-': 3, 'H-': 7, 'O-': 2, 'N-': 1, 'S-': 0},\n",
        "                   'C':{'C-': 3, 'H-': 7, 'O-': 2, 'N-': 1, 'S-': 1},\n",
        "                   'D':{'C-': 4, 'H-': 7, 'O-': 4, 'N-': 1, 'S-': 0},\n",
        "                   'E':{'C-': 5, 'H-': 9, 'O-': 4, 'N-': 1, 'S-': 0},\n",
        "                   'F':{'C-': 9, 'H-': 11,'O-': 2, 'N-': 1, 'S-': 0},\n",
        "                   'G':{'C-': 2, 'H-': 5, 'O-': 2, 'N-': 1, 'S-': 0},\n",
        "                   'H':{'C-': 6, 'H-': 9, 'O-': 2, 'N-': 3, 'S-': 0},\n",
        "                   'I':{'C-': 6, 'H-': 13,'O-': 2, 'N-': 1, 'S-': 0},\n",
        "                   'K':{'C-': 6, 'H-': 14,'O-': 2, 'N-': 2, 'S-': 0},\n",
        "                   'L':{'C-': 6, 'H-': 13,'O-': 2, 'N-': 1, 'S-': 0},\n",
        "                   'M':{'C-': 5, 'H-': 11,'O-': 2, 'N-': 1, 'S-': 1},\n",
        "                   'N':{'C-': 4, 'H-': 8, 'O-': 3, 'N-': 2, 'S-': 0},\n",
        "                   'P':{'C-': 5, 'H-': 9, 'O-': 2, 'N-': 1, 'S-': 0},\n",
        "                   'Q':{'C-': 5, 'H-': 10,'O-': 3, 'N-': 2, 'S-': 0},\n",
        "                   'R':{'C-': 6, 'H-': 14,'O-': 2, 'N-': 4, 'S-': 0},\n",
        "                   'S':{'C-': 3, 'H-': 7, 'O-': 3, 'N-': 1, 'S-': 0},\n",
        "                   'T':{'C-': 4, 'H-': 9, 'O-': 3, 'N-': 1, 'S-': 0},\n",
        "                   'V':{'C-': 5, 'H-': 11,'O-': 2, 'N-': 1, 'S-': 0},\n",
        "                   'W':{'C-': 11,'H-': 12,'O-': 2, 'N-': 2, 'S-': 0},\n",
        "                   'Y':{'C-': 9, 'H-': 11,'O-': 3, 'N-': 1, 'S-': 0}\n",
        "                }\n",
        "\n",
        "def physical_chemical_feature(sequence):\n",
        "  # Implementation from predPHI\n",
        "    seq_new=sequence.replace('X','').replace('U','').replace('B','').replace('Z','')\n",
        "    CE = 'CHONS'\n",
        "\n",
        "    count = Counter(seq_new)\n",
        "    code = []\n",
        "\n",
        "    for c in CE:\n",
        "        abundance_c = 0\n",
        "        for key in count:\n",
        "            num_c = Chemi_stats[key][c]\n",
        "            abundance_c += num_c * count[key]\n",
        "\n",
        "        code.append(abundance_c)\n",
        "    return(code)\n",
        "\n",
        "def molecular_weight(seq):\n",
        "  # Implementation from predPHI\n",
        "    #seq_new=seq.replace('X','').replace('U','').replace('B','').replace('Z','')\n",
        "    analysed_seq = ProteinAnalysis(seq)\n",
        "    analysed_seq.monoisotopic = True\n",
        "    mw = analysed_seq.molecular_weight()\n",
        "    return mw"
      ],
      "metadata": {
        "id": "f49qJalg_FRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deprecrated stuff\n",
        "\n",
        "Ignore this, I've found more efficient implementations"
      ],
      "metadata": {
        "id": "DJ1dZj8QgZWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_genomic_data(accession_ids):\n",
        "    Entrez.email = email\n",
        "    handle = Entrez.efetch(db=\"nuccore\", id=accession_ids, rettype=\"gb\", retmode=\"text\")\n",
        "    records = SeqIO.parse(handle, \"genbank\")\n",
        "    return records\n",
        "\n",
        "def fetch_protein_data(accession_ids):\n",
        "    Entrez.email = email\n",
        "    handle = Entrez.efetch(db=\"protein\", id=accession_ids, rettype=\"gb\", retmode=\"text\")\n",
        "    records = SeqIO.parse(handle, \"gb\")\n",
        "    return records\n",
        "\n",
        "def fetch_protein_sequence(accession):\n",
        "    Entrez.email = email  # Set your email here\n",
        "    handle = Entrez.efetch(db=\"protein\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
        "    record = handle.read()\n",
        "    handle.close()\n",
        "    # The record will contain the FASTA format which includes the sequence header and the sequence itself\n",
        "    return record\n",
        "\n",
        "\n",
        "# probably dont need this FUNCTION\n",
        "def record_to_seq_list(records):\n",
        "  # Go over records and return a list of only the protein AA sequences\n",
        "  sequence_list = []\n",
        "  for rec in records:\n",
        "    sequence_list.append(rec.seq)\n",
        "\n",
        "  return sequence_list\n"
      ],
      "metadata": {
        "id": "m8qivVz0geyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Creation"
      ],
      "metadata": {
        "id": "0ptENsp9dwAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email = 'rnguye20@ucsc.edu'\n",
        "\n",
        "def get_protein_translations(genome_accession, min_length=30, max_length=float('inf'), exclude_hypothetical=True):\n",
        "    # Given a genome accession code, return a list of all protein translations\n",
        "    Entrez.email = email\n",
        "\n",
        "    handle = Entrez.efetch(db=\"nuccore\", id=genome_accession, rettype=\"gb\", retmode=\"text\")\n",
        "    record = SeqIO.read(handle, \"genbank\")\n",
        "    protein_accession_codes = []\n",
        "    protein_translation_sequences = []\n",
        "    for feature in record.features:\n",
        "        if feature.type == \"CDS\" and \"protein_id\" in feature.qualifiers:\n",
        "            protein_id = feature.qualifiers[\"protein_id\"][0]\n",
        "            protein_translation = feature.qualifiers.get(\"translation\", [\"\"])[0]\n",
        "            protein_length = len(protein_translation)\n",
        "            protein_description = feature.qualifiers.get(\"product\", [\"\"])[0]\n",
        "            if exclude_hypothetical and \"hypothetical\" in protein_description.lower(): # exclude hypotheticals\n",
        "                continue\n",
        "            if protein_description.lower() == 'n/a': # exclude proteins that aren't named\n",
        "                continue\n",
        "            if min_length <= protein_length <= max_length: # exclude proteins that are under 30 sequences long\n",
        "                protein_accession_codes.append(protein_id)\n",
        "                protein_translation_sequences.append(protein_translation.replace('X','').replace('U','').replace('B','').replace('Z',''))\n",
        "\n",
        "\n",
        "    if protein_translation_sequences:\n",
        "      return protein_translation_sequences"
      ],
      "metadata": {
        "id": "DbAE0aj4ftdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "## Feature extraction\n",
        "\n",
        "def analyze_seq_list(sequence_list):\n",
        "  # Go over each sequence and return a list of analyzed sequence objects\n",
        "  analyzed_sequences = [ProteinAnalysis(seq) for seq in sequence_list]\n",
        "\n",
        "  return analyzed_sequences\n",
        "\n",
        "def count_CHONS(AAC_dict):\n",
        "  # Given a dictionary of amino acid counts for a sequence, return the elemental composition\n",
        "  # as a pandas dataframe\n",
        "\n",
        "   # Elements denoted by '-' to avoid confusion with amino acid codes\n",
        "  comp_dict = Counter({'C-':0, 'H-':0, 'O-':0, 'N-':0, 'S-':0})\n",
        "\n",
        "  for key, value in AAC_dict.items():\n",
        "    # For a count of amino acid, multiply the count by its known chemical composition\n",
        "    # Then add it to the total\n",
        "    chem_comp_dict = Chemi_stats[key]\n",
        "    multiplied_dict = Counter({AA:element*value for (AA, element) in chem_comp_dict.items()})\n",
        "    comp_dict += multiplied_dict\n",
        "\n",
        "  return pd.DataFrame(comp_dict, index=[0])\n",
        "\n",
        "def dataframe_average(dataframe_list):\n",
        "  # Given a list of dataframes with equal dimensions, return a dataframe\n",
        "  # consisting of element-wise calculated averaged values\n",
        "\n",
        "  averaged_df = sum(dataframe_list)/len(dataframe_list)\n",
        "\n",
        "  labels = list(averaged_df.columns)\n",
        "  avg_label = [x+'_avg' for x in labels] # Label for avg\n",
        "  averaged_df.columns = avg_label\n",
        "\n",
        "  return averaged_df\n",
        "\n",
        "def dataframe_std(dataframe_list):\n",
        "  # Given a list of dataframes with equal dimensions, return a dataframe\n",
        "  # consisting of element-wise calculated standard deviation values\n",
        "\n",
        "  std_combined_df = pd.concat(dataframe_list, axis=0)  # Concatenate along rows\n",
        "  std_inbetween_df = std_combined_df.std(axis=0)\n",
        "  elementwise_std_df = pd.DataFrame(std_inbetween_df)\n",
        "  std_df = pd.DataFrame(elementwise_std_df).T\n",
        "\n",
        "  labels = list(std_df.columns)\n",
        "  std_label = [x+'_std' for x in labels] #label for std\n",
        "  std_df.columns = std_label\n",
        "\n",
        "  return std_df\n",
        "\n",
        "def dataframe_variance(dataframe_list):\n",
        "  # Given a list of dataframes with equal dimensions, return a dataframe\n",
        "  # consisting of element-wise calculated variance values\n",
        "\n",
        "  var_combined_df = pd.concat(dataframe_list, axis=0)  # Concatenate along rows\n",
        "  var_inbetween_df = var_combined_df.var(axis=0)\n",
        "  elementwise_var_df = pd.DataFrame(var_inbetween_df)\n",
        "  var_df = pd.DataFrame(elementwise_var_df).T\n",
        "\n",
        "  labels = list(var_df.columns)\n",
        "  var_label = [x+'_var' for x in labels] #label for var\n",
        "  var_df.columns = var_label\n",
        "\n",
        "  return var_df\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "\n",
        "def feature_extraction(analyzed_list):\n",
        "  # Return features for a phage or a host when given a list of ProteinAnalysis sequence objects\n",
        "  # Returned dataframe is 1x79\n",
        "  DF_seq = []\n",
        "  molecular_weights = []\n",
        "\n",
        "  for an_seq in analyzed_list:\n",
        "    # Get list of each protein's AAP as a dataframe with\n",
        "    # a column for molecular weight and 5 columns for CHONS\n",
        "    AAC_dict = an_seq.count_amino_acids()\n",
        "    AAP_dict = an_seq.get_amino_acids_percent()\n",
        "\n",
        "    mw = an_seq.molecular_weight()\n",
        "\n",
        "    chons_df = count_CHONS(AAC_dict)\n",
        "\n",
        "    dataframed_dict = pd.DataFrame(AAP_dict, index=[0]) # Turn dictionary into dataframe (20 features)\n",
        "    dataframed_dict.insert(len(dataframed_dict.columns), 'MolW', mw) # Add column for mw (1 feature)\n",
        "    final_df = pd.concat([dataframed_dict, chons_df], axis=1) # Add CHONS counts (5 features)\n",
        "    DF_seq.append(final_df) # dataframe is appended to list of dataframes for statistics calculations\n",
        "\n",
        "  # Statistics stuff, applied over the list of dataframes\n",
        "  avg_df = dataframe_average(DF_seq)\n",
        "  std_df = dataframe_std(DF_seq)\n",
        "  var_df = dataframe_variance(DF_seq)\n",
        "\n",
        "  feature_dataframe = pd.concat([avg_df, std_df, var_df], axis=1) #concatenate df horizontally\n",
        "\n",
        "  return feature_dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def relabel_dataframe(df, phage=True):\n",
        "  # Relabel dataframe columns to specify which features belong to phage or host\n",
        "  suffix = '_p' if phage else '_h'\n",
        "  label = list(df)\n",
        "  new_label = [x+suffix for x in label]\n",
        "  df.columns = new_label\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "\n",
        "def accession_to_features(accession, phage=True):\n",
        "  # For some reason this process takes waaay too long\n",
        "  protein_sequence_list = get_protein_translations(accession)\n",
        "\n",
        "  if protein_sequence_list: # If we even got anything from the accession code\n",
        "    analyzed_list = analyze_seq_list(protein_sequence_list)\n",
        "\n",
        "    final_dataframe = feature_extraction(analyzed_list)\n",
        "    relabel_dataframe(final_dataframe, phage) # label dataframe based on whether phage or host\n",
        "\n",
        "    entity_category = 'phage' if phage else 'host' # label for organism type\n",
        "    final_dataframe.insert(0, entity_category, accession)\n",
        "\n",
        "    return final_dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "5fh6ar9peC-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rLg9D6ih6fh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "YdIvTjoshCOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE\n",
        "accession_phage = 'NC_023604'\n",
        "accession_host = 'LC361338'\n",
        "#print(accession_to_features(accession_phage))\n",
        "print(accession_to_features(accession_host, phage=False))\n",
        "#print(get_protein_translations(accession_host))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAkMQDOrofts",
        "outputId": "b25f8c9c-0562-456d-9889-fe35ba29d76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       host   A_avg_h  C_avg_h   D_avg_h   E_avg_h   F_avg_h   G_avg_h  \\\n",
            "0  LC361338  0.151515      0.0  0.047475  0.024242  0.025253  0.129293   \n",
            "\n",
            "    H_avg_h   I_avg_h   K_avg_h  ...  T_var_h  V_var_h  W_var_h  Y_var_h  \\\n",
            "0  0.017172  0.024242  0.009091  ...      NaN      NaN      NaN      NaN   \n",
            "\n",
            "   MolW_var_h  C-_var_h  H-_var_h  O-_var_h  N-_var_h  S-_var_h  \n",
            "0         NaN       NaN       NaN       NaN       NaN       NaN  \n",
            "\n",
            "[1 rows x 79 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab test_set.csv and training_set.csv (list of phage-host interactions) into reads into datafile\n",
        "# Use this to create ALL features for each phage and host\n",
        "accession_url = 'https://raw.githubusercontent.com/xialab-ahu/PredPHI/master/data/test_set.csv'\n",
        "accession2_url = 'https://raw.githubusercontent.com/xialab-ahu/PredPHI/master/data/training_set.csv'\n",
        "test_set = 'test_set.csv'\n",
        "train_set = 'training_set.csv'\n",
        "\n",
        "fields = [\"phage\", \"host\", \"class\"];\n",
        "\n",
        "urllib.request.urlretrieve(accession_url, test_set)\n",
        "df = pd.read_csv(\"test_set.csv\", sep=\"\\t\")\n",
        "\n",
        "urllib.request.urlretrieve(accession2_url, train_set)\n",
        "df2 = pd.read_csv(\"training_set.csv\", sep=\"\\t\")\n",
        "\n",
        "# Just some testing code, ignore\n",
        "interact_list = []\n",
        "phage_list = []\n",
        "host_list = []"
      ],
      "metadata": {
        "id": "Xy6rZSfpdvQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_df = pd.concat([df,df2], axis=0) # HAS ALL INTERACTIONS, USE THIS ONE\n",
        "phage_list = total_df['phage'].tolist()\n",
        "host_list = total_df['host'].tolist()\n",
        "\n",
        "phage_list = set(phage_list)\n",
        "host_list = set(host_list)\n",
        "\n",
        "print(len(phage_list), 'phages')\n",
        "print(len(host_list), 'hosts')\n",
        "print(len(phage_list)+len(host_list), 'phages+hosts')\n",
        "print(len(total_df), 'total interactions')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77uaG3bKjymv",
        "outputId": "40448f43-31e9-4b65-ed33-17ebd711954e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3449 phages\n",
            "301 hosts\n",
            "3750 phages+hosts\n",
            "615340 total interactions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick overview of how it should probably go\n",
        "Ok, so since we have a list of all accession codes for phages and hosts, run accession_to_features(accession, phage=True/False) over all of them, then store the accession code and resulting feature dataframe as a key:value in a dict.\n",
        "\n",
        "After we have the features stored, iterate through the interaction dataframe (total_df) and using the accession codes as keys, grab the feature dataframes of both from the dict then concatenate the dataframes together. Add a column for the classifcation (whether or not they interact).\n",
        "\n",
        "\n",
        ">phage_feat = feature_dict[phage_accession]\n",
        "\n",
        ">host_feat = feature_dict[host_accession]\n",
        "\n",
        ">final_feature_dataframe = df.concat([phage_feat, host_feat], axis=1)\n",
        "\n",
        ">dataframed_dict.insert(0, 'interaction', classification)\n",
        "\n",
        "classification being either a 1 (true) or a 0 (false)\n",
        "\n",
        "With the final, concatenated dataframe, write it into a line in a .csv file. After we run through the entire interactions list, this will be our final data set."
      ],
      "metadata": {
        "id": "pdnO7VTWoyS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This process takes a gazillion years\n",
        "feature_dict = {}\n",
        "accession_errors = []\n",
        "\n",
        "for accession in phage_list.union(host_list):\n",
        "    is_phage = accession in phage_list\n",
        "    try:\n",
        "      feature_dict[accession] = accession_to_features(accession, phage=is_phage)\n",
        "    except Exception as e:\n",
        "      print(\"Had some trouble with this accession code:\", accession)\n",
        "      accession_errors.append(accession)"
      ],
      "metadata": {
        "id": "IrZxv_PYNUxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This process is a little faster\n",
        "final_df_list = []\n",
        "\n",
        "for index, row in total_df.iterrows():\n",
        "  if row['phage'] in feature_dict.keys() and row['host'] in feature_dict.keys():\n",
        "    phage_feat = feature_dict[row['phage']]\n",
        "    host_feat = feature_dict[row['host']]\n",
        "\n",
        "    if isinstance(phage_feat, pd.DataFrame) and isinstance(host_feat, pd.DataFrame):\n",
        "      combined_features_df = pd.concat([phage_feat.reset_index(drop=True),\n",
        "                                        host_feat.reset_index(drop=True)], axis=1)\n",
        "      combined_features_df['interaction'] = row['class']\n",
        "      final_df_list.append(combined_features_df)\n",
        "\n",
        "# Concatenate all interaction feature dataframes into a single dataframe\n",
        "final_features_df = pd.concat(final_df_list, axis=0, ignore_index=True)\n",
        "\n",
        "final_features_df.to_csv('final_interaction_features.csv', index=False)"
      ],
      "metadata": {
        "id": "aSKFw_8nhP-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(final_df_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xoipKCQSn7X",
        "outputId": "a268fac6-0fe4-42c8-d86a-fa3defa54225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "417529\n"
          ]
        }
      ]
    }
  ]
}
